import re
import string
import pandas as pd
from copy import deepcopy
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

df = pd.read_csv('Dataset_Sentimen_Emosi.csv')

df


[3]: Tweet Sentimen Emosi
0 Cegah mata rantai Covid-19,mari kita dirumah s... 1.0 1
1 aku mohon yaAllah semoga wabah covid-19 menghi... 1.0 -1
2 Pemprov Papua Naikkan Status Jadi Tanggap Daru... 1.0 1
3 Covid belum nyampe prigen mbak hmm hoax 0.0 -2
4 Nyuruh orang pintar, lu aja Togog. Itu kerumun... -1.0 -2
.. ... ... ...
899 Seluruh negara di dunia mengalami masa sulit k... 1.0 -2
900 Setelah covid dan skripsi disaster selesai, ma... 1.0 2
901 Malam ini!! Projek "BENDA BOLEH BINCANG" 9 mal... 0.0 1
902 Pontang - panting di koyak covid 19 -1.0 -2
903 Masalahnya sekarang isu jangkitan covid. Alaaa... -1.0 -2
[904 rows x 3 columns]

df = df.drop(['Sentimen'], axis=1)

df.head()

Tweet Emosi
0 Cegah mata rantai Covid-19,mari kita dirumah s... 1
1 aku mohon yaAllah semoga wabah covid-19 menghi... -1
2 Pemprov Papua Naikkan Status Jadi Tanggap Daru... 1
3 Covid belum nyampe prigen mbak hmm hoax -2
4 Nyuruh orang pintar, lu aja Togog. Itu kerumun... -2

from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons
text_processor = TextPreProcessor(
# terms that will be normalized
normalize=['email', 'percent', 'money', 'phone', 'user',
'time', 'date', 'number'],
# terms that will be annotated
#annotate={"hashtag", "allcaps", "elongated", "repeated",'emphasis',␣
,→'censored'},
annotate={"hashtag"},

fix_html=True, # fix HTML tokens
# corpus from which the word statistics are going to be used
# for word segmentation
segmenter="twitter",
# corpus from which the word statistics are going to be used
# for spell correction
corrector="twitter",
unpack_hashtags=True, # perform word segmentation on hashtags
unpack_contractions=True, # Unpack contractions (can't -> can not)
spell_correct_elong=False, # spell correction for elongated words
# select a tokenizer. You can use SocialTokenizer, or pass your own
# the tokenizer, should take as input a string and return a list of tokens
tokenizer=SocialTokenizer(lowercase=True).tokenize,
# list of dictionaries, for replacing tokens extracted from the text,
# with other expressions. You can pass more than one dictionaries.
dicts=[emoticons]
)

def bersih_data(text):
return " ".join(text_processor.pre_process_doc(text))
def non_ascii(text):
return text.encode('ascii', 'replace').decode('ascii')
def remove_space_alzami(text):
return " ".join(text.split())
def remove_emoji_alzami(text):
return ' '.join(re.sub("([x#][A-Za-z0-9]+)"," ", text).split())

def remove_tab(text):
return text.replace('\\t'," ").replace('\\n'," ").replace('\\u'," ").
,→replace('\\',"")
def remove_tab2(text):
return re.sub('\s+',' ',text)
def remove_rt(text):
return text.replace('RT'," ")
def remove_mention(text):
return ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
def remove_incomplete_url(text):
return text.replace("http://", " ").replace("https://", " ")
def remove_single_char(text):
return re.sub(r"\b[a-zA-Z]\b", "", text)
def remove_excessive_dot(text):
return text.replace('..'," ")
def change_stripe(text):
return text.replace('-'," ")
def lower(text):
return text.lower()
def remove_single_char(text):
return re.sub(r"\b[a-zA-Z]\b", "", text)
def remove_excessive_dot(text):
return text.replace('..'," ")
def lower(text):
return text.lower()
def remove_whitespace_LT(text):
return text.strip()
def remove_whitespace_multiple(text):
return re.sub('\s+',' ',text)
def remove_punctuation(text):
remove = string.punctuation
remove = remove.replace("_", "") # don't remove hyphens

pattern = r"[{}]".format(remove) # create the pattern
return re.sub(pattern, "", text)

# hapus untuk <>
def remove_number_eks(text):
return text.replace('<number>'," ")
def remove_angka(text):
return re.sub(r"\d+", "", text)
def remove_URL_eks(text):
return text.replace('URL'," ").replace('url'," ")
def space_punctuation(text):
return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)

i = 0
final_string = []
s = ""
for text in df['Tweet'].values:
filteredSentence = []
EachReviewText = ""
proc = remove_rt(text)
proc = lower(proc)
proc = change_stripe(proc)
proc = remove_emoji_alzami(proc)
proc = remove_tab(proc)
proc = remove_tab2(proc)
proc = non_ascii(proc)
proc = remove_incomplete_url(proc)
proc = remove_excessive_dot(proc)
proc = remove_whitespace_LT(proc)
proc = remove_whitespace_multiple(proc)
proc = remove_single_char(proc)
proc = space_punctuation(proc)
proc = remove_punctuation(proc)
proc = remove_space_alzami(proc)
proc = bersih_data(proc)
proc = remove_number_eks(proc)
proc = remove_angka(proc)
proc = remove_URL_eks(proc)
EachReviewText = proc
final_string.append(EachReviewText)

